{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### *Chest X-Ray Images (Pneumonia) Detector Using CNN and Pytorch*",
   "id": "d635b80e507fddd1"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-16T21:28:14.587112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet34\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pprint\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "h = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 256,\n",
    "    \"image_size\": 224,\n",
    "    \"fc1_size\": 512,\n",
    "    \"lr\": 0.001,\n",
    "    \"model\": \"efficientnetv2\",\n",
    "    \"scheduler\": \"CosineAnnealingLR10\",\n",
    "    \"balance\": True,\n",
    "    \"early_stopping_patience\": float(\"inf\")\n",
    "}\n",
    "\n",
    "def extract_patient_ids(filename):\n",
    "    patient_id = filename.split('_')[0].replace(\"person\", \"\")\n",
    "    return patient_id\n",
    "\n",
    "def split_file_names(input_folder, val_split_perc):\n",
    "    # Pneumonia files contain patient id, so we group split them by patient to avoid data leakage\n",
    "    pneumonia_patient_ids = set([extract_patient_ids(fn) for fn in os.listdir(os.path.join(input_folder, 'PNEUMONIA'))])\n",
    "    pneumonia_val_patient_ids = random.sample(list(pneumonia_patient_ids), int(val_split_perc * len(pneumonia_patient_ids)))\n",
    "\n",
    "    pneumonia_val_filenames = []\n",
    "    pneumonia_train_filenames = []\n",
    "\n",
    "    for filename in os.listdir(os.path.join(input_folder, 'PNEUMONIA')):\n",
    "        patient_id = extract_patient_ids(filename)\n",
    "        if patient_id in pneumonia_val_patient_ids:\n",
    "            pneumonia_val_filenames.append(os.path.join(input_folder, 'PNEUMONIA', filename))\n",
    "        else:\n",
    "            pneumonia_train_filenames.append(os.path.join(input_folder, 'PNEUMONIA', filename))\n",
    "\n",
    "    # Normal (by file, no patient information in file names)\n",
    "    normal_filenames  = [os.path.join(input_folder, 'NORMAL', fn) for fn in os.listdir(os.path.join(input_folder, 'NORMAL'))]\n",
    "    normal_val_filenames = random.sample(normal_filenames, int(val_split_perc * len(normal_filenames)))\n",
    "    normal_train_filenames = list(set(normal_filenames)-set(normal_val_filenames))\n",
    "\n",
    "    train_filenames = pneumonia_train_filenames + normal_train_filenames\n",
    "    val_filenames = pneumonia_val_filenames + normal_val_filenames\n",
    "\n",
    "    return train_filenames, val_filenames\n",
    "\n",
    "def create_weighted_sampler(h, dataset):\n",
    "    targets = dataset.targets\n",
    "    class_counts = np.bincount(targets)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    weights = [class_weights[label] for label in targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "    return sampler\n",
    "\n",
    "class CustomImageFolder(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None, is_valid_file=None):\n",
    "        self.dataset = datasets.ImageFolder(\n",
    "            root, \n",
    "            is_valid_file=is_valid_file\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.targets = self.dataset.targets\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataset[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(\n",
    "                image=np.array(image))[\"image\"]\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def prepare_data(h):\n",
    "    data_transforms_train_alb = A.Compose([\n",
    "        A.Rotate(\n",
    "            limit=20\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ColorJitter(\n",
    "            brightness=0.1, \n",
    "            contrast=0.1, \n",
    "            saturation=0.1, \n",
    "            hue=0.1, \n",
    "            p=1\n",
    "        ),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.1, \n",
    "            scale_limit=0, \n",
    "            rotate_limit=0, \n",
    "            p=0.5\n",
    "        ),\n",
    "        A.Perspective(\n",
    "            scale=(0.05, 0.15), \n",
    "            keep_size=True, \n",
    "            p=0.5\n",
    "        ),\n",
    "        A.Resize(\n",
    "            height=h[\"image_size\"], \n",
    "            width=h[\"image_size\"]\n",
    "        ),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])        \n",
    "\n",
    "    data_transforms_val_alb = A.Compose([\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        A.Resize(\n",
    "            h[\"image_size\"], \n",
    "            h[\"image_size\"]\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])    \n",
    "\n",
    "    # Define the validation split percentage\n",
    "    val_split = 0.2\n",
    "    train_filenames, val_filenames = split_file_names(\n",
    "        \"C:\\\\Users\\\\Омар\\\\PycharmProjects\\\\FinalMLProject\\\\chest_xray\\\\chest_xray\\\\train\\\\\", \n",
    "        val_split\n",
    "    )\n",
    "    # Load the datasets\n",
    "    train_dataset = CustomImageFolder(\n",
    "        \"C:\\\\Users\\\\Омар\\\\PycharmProjects\\\\FinalMLProject\\\\chest_xray\\\\chest_xray\\\\train\\\\\", \n",
    "        transform=data_transforms_train_alb, \n",
    "        is_valid_file=lambda x: x in train_filenames\n",
    "    )\n",
    "    val_dataset = CustomImageFolder(\n",
    "        \"C:\\\\Users\\\\Омар\\\\PycharmProjects\\\\FinalMLProject\\\\chest_xray\\\\chest_xray\\\\train\\\\\", \n",
    "        transform=data_transforms_val_alb, \n",
    "        is_valid_file=lambda x: x in val_filenames\n",
    "    )   \n",
    "    \n",
    "    test_dataset = CustomImageFolder(\n",
    "        \"C:\\\\Users\\\\Омар\\\\PycharmProjects\\\\FinalMLProject\\\\chest_xray\\\\chest_xray\\\\test\\\\\", \n",
    "        transform=data_transforms_val_alb\n",
    "    )\n",
    "    # Create the data loaders for train, validation, and test sets\n",
    "    if (h[\"balance\"]):\n",
    "        sampler = create_weighted_sampler(h, train_dataset)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=h[\"batch_size\"], \n",
    "            sampler=sampler, \n",
    "            num_workers=4\n",
    "        )    \n",
    "    else:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=h[\"batch_size\"], \n",
    "            shuffle=True, \n",
    "            num_workers=4\n",
    "        )    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=h[\"batch_size\"], \n",
    "        shuffle=True, \n",
    "        num_workers=4\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=h[\"batch_size\"], \n",
    "        shuffle=True, \n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def create_model(h, device):\n",
    "    if (h[\"model\"]==\"efficientnetv2\"):\n",
    "        model = timm.create_model(\n",
    "            \"tf_efficientnetv2_b0\", \n",
    "            pretrained=True, \n",
    "            num_classes=2\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        return model  \n",
    "    if (h[\"model\"]==\"fc\"):\n",
    "        model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * h[\"image_size\"] * h[\"image_size\"], h[\"fc1_size\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h[\"fc1_size\"], 2)\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    if (h[\"model\"]==\"cnn\"):\n",
    "        model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, \n",
    "                      padding=1\n",
    "                      ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, 3, \n",
    "                      padding=1\n",
    "                      ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, \n",
    "                      padding=1\n",
    "                      ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(64 * (h[\"image_size\"] // 8) * (h[\"image_size\"] // 8), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    if (h[\"model\"]==\"resnet34\"):\n",
    "        model = resnet34(pretrained=True)\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, 2)\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "\n",
    "def train_model(h, model, train_loader, val_loader, optimizer, criterion, scheduler, device):\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = h[\"early_stopping_patience\"]  # The patience threshold for early stopping\n",
    "\n",
    "    start_time = time.time()\n",
    "    num_epochs = h[\"num_epochs\"]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        progress_bar = tqdm(\n",
    "            train_loader, \n",
    "            desc=f\"Training epoch {epoch + 1}/{num_epochs}\", \n",
    "            leave=False, \n",
    "            unit=\"mini-batch\"\n",
    "        )\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)      \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "        val_loss, _, _, _ = evaluate_model(h, model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Store the loss history\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_loss_history.append(train_loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        # Early stop check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_weights.pth')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}.\")\n",
    "            break        \n",
    "\n",
    "        # Calculate elapsed time and remaining time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_time_per_epoch = elapsed_time / (epoch + 1)\n",
    "        remaining_epochs = num_epochs - (epoch + 1)\n",
    "        remaining_time = avg_time_per_epoch * remaining_epochs\n",
    "\n",
    "        # Convert remaining time to minutes and seconds\n",
    "        remaining_time_min, remaining_time_sec = divmod(remaining_time, 60)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]: Train Loss: {running_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Remaining Time: {remaining_time_min:.0f}m {remaining_time_sec:.0f}s\")\n",
    "\n",
    "    # Loading best model \n",
    "    if patience!=float(\"inf\"):\n",
    "        model.load_state_dict(torch.load('best_model_weights.pth'))\n",
    "\n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "def evaluate_model(h, model, data_loader, criterion, device):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)          \n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    epoch_loss = total_loss / len(data_loader)\n",
    "    epoch_accuracy = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_accuracy, true_labels, predicted_labels\n",
    "def plot_metrics(h, train_loss_history, val_loss_history, test_loss, test_accuracy, true_labels, predicted_labels):\n",
    "    print(f\"Accuracy on the test set: {test_accuracy:.2%}\")\n",
    "\n",
    "    # Calculate precision, recall, and F1 score using the accumulated true labels and predictions\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 score: {f1:.2f}\")\n",
    "\n",
    "    # Calculate the confusion matrix using the accumulated true labels and predictions\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Visualize the confusion matrix\n",
    "    plt.figure()\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm, \n",
    "        display_labels=[\"Normal\", \"Pneumonia\"]\n",
    "    )\n",
    "    disp.plot()\n",
    "\n",
    "    # Plot the learning curves\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss_history, label='Train Loss')\n",
    "    plt.plot(val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss history')\n",
    "    plt.legend()\n",
    "    plt.show()  \n",
    "\n",
    "def create_scheduler(h, optimizer, lr):\n",
    "    scheduler_name = h[\"scheduler\"]\n",
    "    if (scheduler_name==\"\"):\n",
    "        return None\n",
    "    if (scheduler_name==\"CosineAnnealingLR10\"):\n",
    "        return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, \n",
    "            T_max=h[\"num_epochs\"], \n",
    "            eta_min=lr*0.1\n",
    "        )\n",
    "    if (scheduler_name==\"ReduceLROnPlateau5\"):\n",
    "        return torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.1, \n",
    "            patience=5, \n",
    "            verbose=True\n",
    "        )\n",
    "    print (\"Error. Unknown scheduler name '{scheduler_name}'\")\n",
    "    return None\n",
    "\n",
    "def check_solution(h, device, verbose):\n",
    "    train_loader, val_loader, test_loader = prepare_data(h)\n",
    "    model = create_model(h, device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=h[\"lr\"])\n",
    "    scheduler = create_scheduler(h, optimizer, h[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loss_history, val_loss_history = train_model(h, model, train_loader, val_loader, optimizer, criterion, scheduler, device)\n",
    "    test_loss, test_accuracy, true_labels, predicted_labels = evaluate_model(h, model, test_loader, criterion, device)\n",
    "    if verbose:\n",
    "        plot_metrics(h, train_loss_history, val_loss_history, test_loss, test_accuracy, true_labels, predicted_labels)\n",
    "\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "    return f1, test_accuracy\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Get number of CPU cores\n",
    "num_cpu_cores = os.cpu_count()\n",
    "print(f\"Number of CPU cores: {num_cpu_cores}\")\n",
    "\n",
    "# Get GPU name\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU name: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "\n",
    "# Print hyperparameters for records\n",
    "print(\"Hyperparameters:\")\n",
    "pprint.pprint(h)\n",
    "\n",
    "f1_array = np.array([])\n",
    "accuracy_array = np.array([])\n",
    "start_time = time.time()\n",
    "\n",
    "repeats = 10\n",
    "\n",
    "for i in range(repeats):\n",
    "    print(f\"Running solution {i+1}/{repeats}\")\n",
    "    f1, accuracy = check_solution(h, device, verbose=(i==0))\n",
    "    print(f\"F1 = {f1:.2f}, accuracy = {accuracy:.2f} \")\n",
    "    f1_array = np.append(f1_array, f1)\n",
    "    accuracy_array = np.append(accuracy_array, accuracy) \n",
    "\n",
    "# Calculate elapsed time and remaining time\n",
    "repeat_time = (time.time() - start_time) / repeats\n",
    "repeat_time_min, repeat_time_sec = divmod(repeat_time, 60)\n",
    "\n",
    "# Printing final results\n",
    "print(\"Results\")\n",
    "print(f\"F1 List: {f1_array}\")\n",
    "print(f\"Accuracy List: {accuracy_array}\")\n",
    "print(f\"F1: {np.mean(f1_array):.1%} (+-{np.std(f1_array):.1%})\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_array):.1%} (+-{np.std(accuracy_array):.1%})\")\n",
    "print(f\"Time of one solution: {repeat_time_min:.0f}m {repeat_time_sec:.0f}s\")\n",
    "\n",
    "print(f\" | {np.mean(f1_array):.1%} (+-{np.std(f1_array):.1%}) | {np.mean(accuracy_array):.1%} (+-{np.std(accuracy_array):.1%}) | {repeat_time_min:.0f}m {repeat_time_sec:.0f}s\")\n",
    "\n",
    "# Print hyperparameters for reminding what the final data is fore\n",
    "print(\"Hyperparameters:\")\n",
    "pprint.pprint(h)"
   ],
   "id": "fe362cbea6b9ee4e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Омар\\PycharmProjects\\FinalMLProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Number of CPU cores: 8\n",
      "No GPU available\n",
      "Hyperparameters:\n",
      "{'balance': True,\n",
      " 'batch_size': 256,\n",
      " 'early_stopping_patience': inf,\n",
      " 'fc1_size': 512,\n",
      " 'image_size': 224,\n",
      " 'lr': 0.001,\n",
      " 'model': 'efficientnetv2',\n",
      " 'num_epochs': 10,\n",
      " 'scheduler': 'CosineAnnealingLR10'}\n",
      "Running solution 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1/10:   0%|          | 0/17 [00:00<?, ?mini-batch/s]"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
